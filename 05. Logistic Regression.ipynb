{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Binary Logistic Regression\n",
    "You can access videos of the derivation of Logistic Regression here: https://youtu.be/FGnoHdjFrJ8.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# or play it in the browser right here! \n",
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"FGnoHdjFrJ8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "From the videos, the main result is the *update equation* for **binary logistic regression**. The update equation for logistic regression can be written as:\n",
    "\n",
    "$$ \\underbrace{w_j}_{\\text{new value}} \\leftarrow \\underbrace{w_j}_{\\text{old value}} + \\eta \\underbrace{\\sum_{i=1}^M (y^{(i)}-g(x^{(i)}))x^{(i)}_j}_{\\text{gradient}} $$\n",
    "\n",
    "where $\\eta$ is the learning rate, $g(\\cdot)$ is the sigmoid function, $y^{(i)}$ is an instance of the target, $\\hat{y}^{(i)}$ is an instance of the prediction (for the current value of $w$), and $x^{(i)}_j$ is the $i^{th}$ instance of the input with the $j^{th}$ feature (that is, the feature in row $i$ and column $j$). The process is iterative, so we must calculate the gradient every time $w$ is updated. We derived this for the simple gradient ascent approach, where we calculate the gradient and step in the direction of steepest ascent. \n",
    "\n",
    "We can simplify this calculation a bit using matrix algebra to solve for the entire vector $w$:\n",
    "\n",
    "$$ w \\leftarrow w + \\eta \\sum_{i=1}^M (y^{(i)}-g(x^{(i)}))x^{(i)} $$\n",
    "\n",
    "where we simply multiply the entire $x^{(i)}$ instance by the scalar difference, $y^{(i)}-\\hat{y}^{(i)}$. \n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now lets start to program logistic regression in the binary case. Recall that the logistic regression equation directly regresses the logistic function (also called a sigmoid) for the probability $y=1$. In this case,\n",
    "\n",
    "$$ p(y^{(i)}=1\\text{ | }x^{(i)},w)=\\frac{1}{1+\\exp{(w^T x^{(i)}})}$$\n",
    "\n",
    "Recall also that the dot product of $w^T x^{(i)}$ results in a scalar value. That is, we multiply $w$ and $x^{(i)}$ together to get a single value, then place that single value through the sigmoid to get the probability that $y=1$. The prediction can be written as:\n",
    "\n",
    "$$ \\hat{y}^{(i)} = 1 \\text{      if,  } p(y^{(i)}=1\\text{ | }x^{(i)},w) > 0.5  $$ and $$ \\hat{y}^{(i)} = 0 \\text{      otherwise  } $$\n",
    "\n",
    "___\n",
    "Let's start by programming the sigmoid and predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class BinaryLogisticRegressionBase:\n",
    "    # private:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Base Binary Logistic Regression Object, Not Trainable'\n",
    "    \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        return 1/(1+np.exp(-theta)) \n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "        \n",
    "blr = BinaryLogisticRegressionBase(0.1)\n",
    "print(blr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "Now let's add the model training function, using the update equation defined previously:\n",
    "\n",
    "$$ w \\leftarrow w + \\eta \\sum_{i=1}^M (y^{(i)}-g(x^{(i)}))x^{(i)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inherit from base class\n",
    "class BinaryLogisticRegression(BinaryLogisticRegressionBase):\n",
    "    #private:\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    def _get_gradient(self,X,y):\n",
    "        # programming \\sum_i (yi-g(xi))xi\n",
    "        gradient = np.zeros(self.w_.shape) # set gradient to zero\n",
    "        for (xi,yi) in zip(X,y):\n",
    "            gradi = (yi - self.predict_proba(xi,add_bias=False))*xi # the actual update inside of sum\n",
    "            gradient += gradi.reshape(self.w_.shape) # reshape to be column vector and add to gradient\n",
    "        \n",
    "        return gradient/float(len(y))\n",
    "       \n",
    "    # public:\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "\n",
    "            \n",
    "blr = BinaryLogisticRegression(0.1)\n",
    "print(blr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how our classifier works, we need some data. Let's first lets load in some example data, the iris dataset.\n",
    "\n",
    "Notice that we have made the iris classification problem **binary**. Instead of three classes, we have made it only two classes. Most values are zero (as seen in the pie chart above). While only about a third of all instances are of class type \"1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import plotly\n",
    "\n",
    "ds = load_iris()\n",
    "X = ds.data\n",
    "y = (ds.target>1).astype(np.int) # make problem binary\n",
    "\n",
    "plotly.offline.init_notebook_mode() # run at the start of every notebook\n",
    "\n",
    "graph1 = {'labels': np.unique(y),\n",
    "          'values': np.bincount(y),\n",
    "            'type': 'pie'}\n",
    "fig = dict()\n",
    "fig['data'] = [graph1]\n",
    "fig['layout'] = {'title': 'Binary Class Distribution',\n",
    "                'autosize':False,\n",
    "                'width':500,\n",
    "                'height':300}\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Now we can train the classifier\n",
    "blr = BinaryLogisticRegression(0.1,10)\n",
    "\n",
    "blr.fit(X,y)\n",
    "print(blr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<br>\n",
    "<div class=\"alert alert-warning\">\n",
    "### Self Test Question\n",
    "What part of the equation is represented by this piece of code:\n",
    "\n",
    "    self.predict_proba(xi)\n",
    "    \n",
    "$$ w \\leftarrow w + \\eta \\sum_{i=1}^M \\underbrace{(\\underbrace{y^{(i)}}_{\\bf A}-\\underbrace{g(x^{(i)})}_{\\bf B})}_{\\bf D}\\underbrace{x^{(i)}}_{\\bf C} $$\n",
    "</div>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how well did the training go?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "yhat = blr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# can we do better? Maybe more iterations?\n",
    "params = dict(eta=0.1,\n",
    "              iterations=500)\n",
    "\n",
    "blr = BinaryLogisticRegression(**params)\n",
    "blr.fit(X,y)\n",
    "print(blr)\n",
    "yhat = blr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending to Vectorized Programming\n",
    "Excellent! We programmed the correct update equation. But, it seems to be really slow. That's because python is not fast with `for` loops. We can speed up the process using vectorization in numpy. This is fast because numpy is written in c++ under the hood. Let's change our update equation to use only linear algebra and fewer summations (which are for loops).  \n",
    "\n",
    "Lets start with only using $X$, instead of $x^{(i)}$. $X$ is the matrix of all instances in each row: \n",
    "\n",
    "$$ X=\\begin{bmatrix}\n",
    "        \\leftarrow &  x^{(1)}      & \\rightarrow  \\\\\n",
    "        \\leftarrow &  x^{(2)}      & \\rightarrow  \\\\\n",
    "        &  \\vdots &\\\\\n",
    "        \\leftarrow &  x^{(M)}      & \\rightarrow  \\\\\n",
    "     \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let's also create a matrix of the differences between the target and prediction:\n",
    "$$ y_{diff}=\\begin{bmatrix}\n",
    "        y^{(1)}-g(x^{(1)})  \\\\\n",
    "        y^{(2)}-g(x^{(2)})  \\\\\n",
    "        \\vdots \\\\\n",
    "        y^{(M)}-g(x^{(M)})  \\\\\n",
    "     \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "What we really want to do is scale each row of $X$ by the corresponding scalar value of $y_{diff}$. Luckily for us, this is exactly how `numpy` interprets multiplication of a matrix with with a column vector that has the same number of rows. So we can simply write \n",
    "\n",
    "$$gradient =\\sum_{i=1}^M (y^{(i)}-g(x^{(i)}))x^{(i)}$$\n",
    "\n",
    "as\n",
    "\n",
    "$$gradient = X*y_{diff}$$\n",
    "\n",
    "where $*$ is a row-wise operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# now lets do some vectorized coding\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "class VectorBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # inherit from our previous class to get same functionality\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # but overwrite the gradient calculation\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        return gradient.reshape(self.w_.shape)\n",
    "\n",
    "# use same params as defined above\n",
    "blr = VectorBinaryLogisticRegression(**params)\n",
    "blr.fit(X,y)\n",
    "print(blr.w_)\n",
    "yhat = blr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"alert alert-warning\">\n",
    "### Self Test Question: \n",
    "If we had a class imbalance problem, with one type of class dominating the response, how could we manipulate the above `fit` method to give more importance to a given class?\n",
    "- **A.** Add weights to the gradient calculation (for each instance of ydiff)\n",
    "- **B.** Repeat certain classes in the update of the gradient calculation\n",
    "- **C.** Update objective function to only look at performance on the worst performing class\n",
    "\n",
    "</div>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Multiclass Logistic Regression\n",
    "Our previous methods only worked for a single class, to extend this beyond binary classification we can use a common trick: train a separate Binary Classifier for each unique class. During prediction, we run an instance through each Binary Classifier, choosing the Binary Classifier that has the highest probability. **This is a method known as one-versus-all** because we only assume one class is positive when we train each separate Binary Classifier. \n",
    "\n",
    "This graphic breaks down the process:\n",
    "![OVA](http://www.saberismywife.com/2016/11/29/Machine-Learning-3/21.png)\n",
    "\n",
    "To program this, its surprisingly simple now that we have a fast binary classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            blr = VectorBinaryLogisticRegression(self.eta,self.iters)\n",
    "            blr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "            probs.append(blr.predict_proba(X)) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n",
    "    \n",
    "lr = LogisticRegression(0.1,1500)\n",
    "print(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = load_iris()\n",
    "X = ds.data\n",
    "y = ds.target # note problem is NOT binary anymore, there are three classes!\n",
    "\n",
    "lr = LogisticRegression(0.1,500)\n",
    "lr.fit(X,y)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing to Scikit Learn\n",
    "Of course, there was no **need** to program the above except for the learning experience (or because we want to extend the learning algorithm without adjusting the scikit-learn source code). Now, let's use the builtin API for `sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "\n",
    "lr_sk = SKLogisticRegression() # all params default\n",
    "\n",
    "lr_sk.fit(X,y)\n",
    "print(np.hstack((lr_sk.intercept_[:,np.newaxis],lr_sk.coef_)))\n",
    "yhat = lr_sk.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this so much faster? The answer is parallelization is better handled through sklearn, separating out each classification, and the operations are ALL in lower level c++. **The main reason, however, is that  the update used is more advanced than what we have talked about.** They have an array of solvers that use the gradient (and Hessian) to solve the optimization more quickly. The accuracy is higher because this logistic regression object uses regularization, which helps control for over-learning, and also because the optimization allows for finer steps when we get to places where the gradient is small and the objective function is less well behaved. \n",
    "____\n",
    "## Logistic Regression Class Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# linear boundaries visualization from sklearn documentation\n",
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_decision_boundaries(lr,Xin,y,title=''):\n",
    "    Xb = copy.deepcopy(Xin)\n",
    "    lr.fit(Xb[:,:2],y) # train only on two features\n",
    "\n",
    "    h=0.01\n",
    "    # create a mesh to plot in\n",
    "    x_min, x_max = Xb[:, 0].min() - 1, Xb[:, 0].max() + 1\n",
    "    y_min, y_max = Xb[:, 1].min() - 1, Xb[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # get prediction values\n",
    "    Z = lr.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.5)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(Xb[:, 0], Xb[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "    plt.xlabel('Sepal length')\n",
    "    plt.ylabel('Sepal width')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "lr = LogisticRegression(0.1,1500) # this is still OUR LR implementation\n",
    "plot_decision_boundaries(lr,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we control these boundaries?\n",
    "Let's introduce another parameter in the objective function called the L2-Norm. The idea here is that we want to keep the values of the weights relatively small. This helps to control overfitting to the data and thus . We are changing our objective function by adding in a new summation of the weights:\n",
    "\n",
    "$$ l(w)_{reg} = l(w)_{old} + C\\cdot\\sum_j w_j^2 $$\n",
    "\n",
    "This means the gradient will be updated as follows:\n",
    "\n",
    "$$ \\underbrace{w_j}_{\\text{new value}} \\leftarrow \\underbrace{w_j}_{\\text{old value}} + \\eta \\underbrace{\\left[\\left(\\sum_{i=1}^M (y^{(i)}-g(x^{(i)}))x^{(i)}_j\\right) + C \\cdot 2w_j \\right]}_{\\text{new gradient}}$$\n",
    "\n",
    "Which is the same as:\n",
    "$$ w \\leftarrow w + \\eta \\left[\\underbrace{\\nabla l(w)_{old}}_{\\text{old gradient}} + C \\cdot 2w \\right]$$\n",
    "\n",
    "Its fairly easy to implement because we just need to add to the existing gradient function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RegularizedBinaryLogisticRegression(VectorBinaryLogisticRegression):\n",
    "    # extend init functions\n",
    "    def __init__(self, C=0.0, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.C = C\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds) # call parent initializer\n",
    "        \n",
    "        \n",
    "    # extend previous class to change functionality\n",
    "    def _get_gradient(self,X,y):\n",
    "        # call get gradient from previous class\n",
    "        gradient = super()._get_gradient(X,y)\n",
    "        \n",
    "        # add in regularization (to all except bias term)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        return gradient\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now redefine the Logistic Regression Function where needed\n",
    "class RegularizedLogisticRegression(LogisticRegression):\n",
    "    def __init__(self, C=0.0, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.C = C\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds) # call parent initializer\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            blr = RegularizedBinaryLogisticRegression(eta=self.eta,\n",
    "                                                      iterations=self.iters,\n",
    "                                                      C=self.C)\n",
    "            blr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import widgets as wd\n",
    "\n",
    "cost_vals = np.logspace(-3,-2,15)\n",
    "def lr_explor(cost_idx):\n",
    "    C = cost_vals[cost_idx]\n",
    "    lr_clf = RegularizedLogisticRegression(eta=0.1,\n",
    "                                           iterations=2500,\n",
    "                                           C=C) # get object\n",
    "    \n",
    "    plot_decision_boundaries(lr_clf,X,y,title=\"C=%.5f\"%(C))\n",
    "\n",
    "wd.interact(lr_explor,cost_idx=(0,15,1),__manual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Extended Logistic Regression Example\n",
    "\n",
    "In this example we will explore methods of using logistic regression in scikit-learn. A basic understanding of scikit-learn is required to complete this notebook, but we start very basic. Note also that there are more efficient methods of separating testing and training data, but we will leave that for a later lecture.\n",
    "\n",
    "First let's load a dataset and prepare it for analysis. We will use pandas to load in data, and then prepare it for classification. We will be using the titanic dataset (a very modest sized data set of about 1000 instances). The imputation methods used here are discussed in a previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/titanic.csv') # read in the csv file\n",
    "\n",
    "# 1. Remove attributes that just arent useful for us\n",
    "del df['PassengerId']\n",
    "del df['Name']\n",
    "del df['Cabin']\n",
    "del df['Ticket']\n",
    "\n",
    "# 2. Impute some missing values, grouped by their Pclass and SibSp numbers\n",
    "df_grouped = df.groupby(by=['Pclass','SibSp'])\n",
    "\n",
    "# # now use this grouping to fill the data set in each group, then transform back\n",
    "# fill in the numeric values\n",
    "df_imputed = df_grouped.transform(lambda grp: grp.fillna(grp.median()))\n",
    "# fill in the categorical values\n",
    "df_imputed[['Sex','Embarked']] = df_grouped[['Sex','Embarked']].apply(lambda grp: grp.fillna(grp.mode()))\n",
    "# fillin the grouped variables from original data frame\n",
    "df_imputed[['Pclass','SibSp']] = df[['Pclass','SibSp']]\n",
    "\n",
    "# 4. drop rows that still had missing values after grouped imputation\n",
    "df_imputed.dropna(inplace=True)\n",
    "\n",
    "# 5. Rearrange the columns\n",
    "df_imputed = df_imputed[['Survived','Age','Sex','Parch','SibSp','Pclass','Fare','Embarked']]\n",
    "\n",
    "df_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# perform one-hot encoding of the categorical data \"embarked\"\n",
    "tmp_df = pd.get_dummies(df_imputed.Embarked,prefix='Embarked')\n",
    "df_imputed = pd.concat((df_imputed,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "# replace the current Sex atribute with something slightly more intuitive and readable\n",
    "df_imputed['IsMale'] = df_imputed.Sex=='male' \n",
    "df_imputed.IsMale = df_imputed.IsMale.astype(np.int)\n",
    "\n",
    "# Now let's clean up the dataset\n",
    "if 'Sex' in df_imputed:\n",
    "    del df_imputed['Sex'] # if 'Sex' column still exists, delete it (as we created an ismale column)\n",
    "    \n",
    "if 'Embarked' in df_imputed:    \n",
    "    del df_imputed['Embarked'] # get reid of the original category as it is now one-hot encoded\n",
    "    \n",
    "# Finally, let's create a new variable based on the number of family members\n",
    "# traveling with the passenger\n",
    "\n",
    "# notice that this new column did not exist before this line of code--we use the pandas \n",
    "#    syntax to add it in \n",
    "df_imputed['FamilySize'] = df_imputed.Parch + df_imputed.SibSp\n",
    "df_imputed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Split\n",
    "For training and testing purposes, let's gather the data we have and grab 80% of the instances for training and the remaining 20% for testing. Moreover, let's repeat this process of separating the testing and training data three times. We will use the hold out cross validation method built into scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'Survived' in df_imputed:\n",
    "    y = df_imputed['Survived'].values # get the labels we want\n",
    "    del df_imputed['Survived'] # get rid of the class label\n",
    "    norm_features = ['Age','Fare' ]\n",
    "    df_imputed[norm_features] = (df_imputed[norm_features]-df_imputed[norm_features].mean()) / df_imputed[norm_features].std()\n",
    "    X = df_imputed.values # use everything else to predict!\n",
    "\n",
    "    ## X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    #    have converted them into simple matrices to use with scikit learn\n",
    "    \n",
    "    \n",
    "# to use the cross validation object in scikit learn, we need to grab an instance\n",
    "#    of the object and set it up. This object will be able to split our data into \n",
    "#    training and testing splits\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(\n",
    "                         n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "lr_clf = RegularizedLogisticRegression(eta=0.1,iterations=2000) # get object\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "    \n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this does the exact same thing as the above block of code, but with shorter syntax\n",
    "\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "    lr_clf.fit(X[train_indices],y[train_indices])  # train object\n",
    "    y_hat = lr_clf.predict(X[test_indices]) # get test set precitions\n",
    "\n",
    "    # print the accuracy and confusion matrix \n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", mt.accuracy_score(y[test_indices],y_hat)) \n",
    "    print(\"confusion matrix\\n\",mt.confusion_matrix(y[test_indices],y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import widgets as wd\n",
    "\n",
    "num_cv_iterations = 10\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.5)\n",
    "\n",
    "def lr_explor(cost):\n",
    "    print('Running')\n",
    "    lr_clf = RegularizedBinaryLogisticRegression(eta=0.05,\n",
    "                                                 iterations=1000,\n",
    "                                                 C=float(cost)) # get object\n",
    "    acc = []\n",
    "    for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "        lr_clf.fit(X[train_indices],y[train_indices])  # train object\n",
    "        y_hat = lr_clf.predict(X[test_indices]) # get test set predictions\n",
    "        acc.append(mt.accuracy_score(y[test_indices],y_hat))\n",
    "        \n",
    "    acc = np.array(acc)\n",
    "    print(acc.mean(),'+-',2*acc.std())\n",
    "        \n",
    "wd.interact(lr_explor,cost=list(np.logspace(-4,1,15)),__manual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:MLEnv]",
   "language": "python",
   "name": "conda-env-MLEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
