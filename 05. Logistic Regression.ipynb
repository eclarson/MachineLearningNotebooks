{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Binary Logistic Regression\n",
    "You can access videos of the derivation of Logistic Regression here: https://youtu.be/FGnoHdjFrJ8.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or play it in the browser right here! \n",
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"FGnoHdjFrJ8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "The objective function that we defined was (if we convert to take mean, rather than the sum):\n",
    "\n",
    "\n",
    "$$ l(\\mathbf{w}) = \\frac{1}{M}\\cdot\\sum_i \\left( y^{(i)} \\ln [g(\\mathbf{w}^T \\mathbf{x}^{(i)})] + (1-y^{(i)}) (\\ln [1 - g(\\mathbf{w}^T \\mathbf{x}^{(i)})])  \\right)  $$\n",
    "\n",
    "From the videos, the main result is the *update equation* for **binary logistic regression**. The update equation for logistic regression can be written generically as:\n",
    "\n",
    "$$ w_j \\leftarrow w_j + \\eta \\nabla l(\\mathbf{w}) $$\n",
    "\n",
    "And more specifically as:\n",
    "$$ \\underbrace{w_j}_{\\text{new value}} \\leftarrow \\underbrace{w_j}_{\\text{old value}} + \\frac{\\eta}{M} \\underbrace{\\sum_{i=1}^M (y^{(i)}-g(\\mathbf{w}^T\\mathbf{x}^{(i)}))x^{(i)}_j}_{\\text{gradient}} $$\n",
    "\n",
    "where $\\eta$ is the learning rate, $g(\\cdot)$ is the sigmoid function, $y^{(i)}$ is an instance of the target, $\\hat{y}^{(i)}$ is an instance of the prediction (for the current value of $w$), and $x^{(i)}_j$ is the $i^{th}$ instance of the input with the $j^{th}$ feature (that is, the feature in row $i$ and column $j$). The process is iterative, so we must calculate the gradient every time $w$ is updated. We derived this for the simple gradient ascent approach, where we calculate the gradient and step in the direction of steepest ascent. \n",
    "\n",
    "We can simplify this calculation a bit using matrix algebra to solve for the entire vector $w$:\n",
    "\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} + \\frac{\\eta}{M} \\sum_{i=1}^M (y^{(i)}-g(\\mathbf{w}^T\\mathbf{x}^{(i)}))\\cdot\\mathbf{x}^{(i)} $$\n",
    "\n",
    "where we simply multiply the entire $\\mathbf{x}^{(i)}$ instance by the scalar difference, $y^{(i)}-\\widehat{y}^{(i)}$. \n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now lets start to program logistic regression in the binary case. Recall that the logistic regression equation directly regresses the logistic function (also called a sigmoid) for the probability $y=1$. In this case,\n",
    "\n",
    "$$ p(y^{(i)}=1\\text{ | }\\mathbf{x}^{(i)},\\mathbf{w})=g(\\mathbf{w}^T \\mathbf{x}^{(i)})=\\frac{1}{1+\\exp{(-\\mathbf{w}^T \\mathbf{x}^{(i)}})}$$\n",
    "\n",
    "Recall also that the dot product of $w^T x^{(i)}$ results in a scalar value. That is, we multiply $w$ and $x^{(i)}$ together to get a single value, then place that single value through the sigmoid to get the probability that $y=1$. The prediction can be written as:\n",
    "\n",
    "$$ \\widehat{y}^{(i)} = 1 \\text{      if,  } g(\\mathbf{w}^T \\mathbf{x}^{(i)}) > 0.5  $$ and $$ \\widehat{y}^{(i)} = 0 \\text{      otherwise  } $$\n",
    "\n",
    "___\n",
    "Let's start by programming the sigmoid and predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class BinaryLogisticRegressionBase:\n",
    "    # private:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Base Binary Logistic Regression Object, Not Trainable'\n",
    "    \n",
    "    # convenience, private and static:\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        return 1/(1+np.exp(-theta)) \n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_intercept(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self, X, add_intercept=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_intercept(X) if add_intercept else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "        \n",
    "blr = BinaryLogisticRegressionBase(0.1)\n",
    "print(blr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "Now let's add the model training function, using the update equation defined previously:\n",
    "\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\frac{1}{M}\\sum_{i=1}^M (y^{(i)}-g(\\mathbf{w}^T\\mathbf{x}^{(i)}))\\mathbf{x}^{(i)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inherit from base class\n",
    "class BinaryLogisticRegression(BinaryLogisticRegressionBase):\n",
    "    #private:\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "\n",
    "\n",
    "    @property\n",
    "    def coef_(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return self.w_[1:]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    @property\n",
    "    def intercept_(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return self.w_[0]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        \n",
    "    def _get_gradient(self,X,y):\n",
    "        # programming \\sum_i (yi-g(xi))xi\n",
    "        gradient = np.zeros(self.w_.shape) # set gradient to zero\n",
    "        for (xi,yi) in zip(X,y):\n",
    "            # the actual update inside of sum\n",
    "            gradi = (yi - self.predict_proba(xi,add_intercept=False))*xi \n",
    "            # reshape to be column vector and add to gradient\n",
    "            gradient += gradi.reshape(self.w_.shape) \n",
    "        \n",
    "        return gradient/float(len(y))\n",
    "       \n",
    "    # public:\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_intercept(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "\n",
    "            \n",
    "blr = BinaryLogisticRegression(0.1)\n",
    "print(blr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how our classifier works, we need some data. Let's first lets load in some example data, the iris dataset.\n",
    "\n",
    "Notice that we have made the iris classification problem **binary**. Instead of three classes, we have made it only two classes. Most values are zero (as seen in the pie chart above). While only about a third of all instances are of class type \"1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import plotly\n",
    "\n",
    "ds = load_iris()\n",
    "X = ds.data\n",
    "y = (ds.target>1).astype(int) # make problem binary\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.8, test_size=0.2)\n",
    "\n",
    "plotly.offline.init_notebook_mode() # run at the start of every notebook\n",
    "\n",
    "graph1 = {'x': np.unique(y),\n",
    "          'y': np.bincount(y)/len(y),\n",
    "            'type': 'bar'}\n",
    "\n",
    "fig = dict()\n",
    "fig['data'] = [graph1]\n",
    "fig['layout'] = {'title': 'Binary Class Distribution',\n",
    "                'autosize':False,\n",
    "                'width':400,\n",
    "                'height':400}\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Now we can train the classifier\n",
    "blr = BinaryLogisticRegression(eta=0.1,iterations=12)\n",
    "blr.fit(X_train,y_train)\n",
    "print(blr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"coef\",blr.coef_)\n",
    "print(\"intercept\",blr.intercept_)\n",
    "print(\"w\",blr.w_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "\n",
    "### Self Test Question\n",
    "What part of the equation is represented by this piece of code:\n",
    "\n",
    "    self.predict_proba(xi)\n",
    "    \n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\frac{1}{M}\\sum_{i=1}^M \\underbrace{(\\underbrace{y^{(i)}}_{\\bf A}-\\underbrace{g(\\mathbf{w}^T \\mathbf{x}^{(i)})}_{\\bf B})}_{\\bf D}\\underbrace{\\mathbf{x}^{(i)}}_{\\bf C} $$\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how well did the training go?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "yhat = blr.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# can we do better? Maybe more iterations?\n",
    "params = dict(eta=0.01,\n",
    "              iterations=500)\n",
    "\n",
    "blr = BinaryLogisticRegression(**params)\n",
    "blr.fit(X_train,y_train)\n",
    "print(blr)\n",
    "yhat = blr.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending to Vectorized Programming\n",
    "Excellent! We programmed the correct update equation. But, it seems to be really slow. That's because python is not fast with `for` loops. We can speed up the process using vectorization in numpy. This is fast because numpy is written in c++ under the hood. Let's change our update equation to use only linear algebra and fewer summations (which are for loops).  \n",
    "\n",
    "Lets start with only using $X$, instead of $x^{(i)}$. $X$ is the matrix of all instances in each row: \n",
    "\n",
    "$$ \\mathbf{X}=\\begin{bmatrix}\n",
    "        \\leftarrow &  \\mathbf{x}^{(1)}      & \\rightarrow  \\\\\n",
    "        \\leftarrow &  \\mathbf{x}^{(2)}      & \\rightarrow  \\\\\n",
    "        &  \\vdots &\\\\\n",
    "        \\leftarrow &  \\mathbf{x}^{(M)}      & \\rightarrow  \\\\\n",
    "     \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let's also create a matrix of the differences between the target and prediction:\n",
    "$$ \\mathbf{y}_{diff}=\\begin{bmatrix}\n",
    "        y^{(1)}-g(\\mathbf{w}^T\\mathbf{x}^{(1)})  \\\\\n",
    "        y^{(2)}-g(\\mathbf{w}^T\\mathbf{x}^{(2)})  \\\\\n",
    "        \\vdots \\\\\n",
    "        y^{(M)}-g(\\mathbf{w}^T\\mathbf{x}^{(M)})  \\\\\n",
    "     \\end{bmatrix}=\\begin{bmatrix}\n",
    "        y_{diff}^{(1)}  \\\\\n",
    "        y_{diff}^{(2)}  \\\\\n",
    "        \\vdots \\\\\n",
    "        y_{diff}^{(M)}  \\\\\n",
    "     \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "What we really want to do is scale each row of $\\mathbf{X}$ by the corresponding scalar value of $\\mathbf{y}_{diff}$. Luckily for us, this is exactly how `numpy` interprets multiplication of a matrix with with a column vector that has the same number of rows. So we can simply write \n",
    "\n",
    "$$\\nabla l(\\mathbf{w}) =\\frac{1}{M}\\sum_{i=1}^M (y^{(i)}-g(\\mathbf{w}^T\\mathbf{x}^{(i)}))\\mathbf{x}^{(i)}$$\n",
    "\n",
    "as\n",
    "\n",
    "$$\\nabla l(\\mathbf{w}) = \\text{mean}(\\mathbf{y}_{diff}\\odot\\mathbf{X})_{columns}$$\n",
    "\n",
    "where $\\odot$ is a row-wise operator that multiplies the scalar value in each row of $\\mathbf{y}_{diff}$ to each element in the same row as $\\mathbf{X}$.\n",
    "\n",
    "$$ \\mathbf{y}_{diff}\\odot\\mathbf{X}=\\begin{bmatrix}\n",
    "        y_{diff}^{(1)}  \\\\\n",
    "        y_{diff}^{(2)}  \\\\\n",
    "        \\vdots \\\\\n",
    "        y_{diff}^{(M)}  \\\\\n",
    "     \\end{bmatrix}\\cdot\n",
    "\\begin{bmatrix}\n",
    "        \\leftarrow &  \\mathbf{x}^{(1)}      & \\rightarrow  \\\\\n",
    "        \\leftarrow &  \\mathbf{x}^{(2)}      & \\rightarrow  \\\\\n",
    "        &  \\vdots &\\\\\n",
    "        \\leftarrow &  \\mathbf{x}^{(M)}      & \\rightarrow  \\\\\n",
    "     \\end{bmatrix}=\\begin{bmatrix}\n",
    "        (\\leftarrow &  \\mathbf{x}^{(1)}      & \\rightarrow) \\cdot y_{diff}^{(1)} \\\\\n",
    "        (\\leftarrow &  \\mathbf{x}^{(2)}      & \\rightarrow) \\cdot y_{diff}^{(2)} \\\\\n",
    "        &  \\vdots &\\\\\n",
    "        (\\leftarrow &  \\mathbf{x}^{(M)}      & \\rightarrow) \\cdot y_{diff}^{(M)} \\\\\n",
    "        \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# now lets do some vectorized coding\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "class VectorBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # inherit from our previous class to get same functionality\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # but overwrite the gradient calculation\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_intercept=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        return gradient.reshape(self.w_.shape)\n",
    "\n",
    "# use same params as defined above\n",
    "blr = VectorBinaryLogisticRegression(**params)\n",
    "blr.fit(X_train,y_train)\n",
    "print(blr.w_)\n",
    "yhat = blr.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Self Test Question: \n",
    "If we had a class imbalance problem, with one type of class dominating the response, how could we manipulate the above `fit` method to give more importance to a given class?\n",
    "- **A.** Add weights to the gradient calculation (for each instance of ydiff)\n",
    "- **B.** Repeat certain classes in the update of the gradient calculation\n",
    "- **C.** Update objective function to only look at performance on the worst performing class\n",
    "\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Multiclass Logistic Regression\n",
    "Our previous methods only worked for a single class, to extend this beyond binary classification we can use a common trick: train a separate Binary Classifier for each unique class. During prediction, we run an instance through each Binary Classifier, choosing the Binary Classifier that has the highest probability. **This is a method known as one-versus-all** because we only assume one class is positive when we train each separate Binary Classifier. \n",
    "\n",
    "This graphic breaks down the process:\n",
    "- graphic from: https://antonhaugen.medium.com/introducing-mllibs-one-vs-rest-classifier-402eeab22493\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*4Ii3aorSLU50RV6V5xalzg.png\" alt=\"OVA\" width=\"500\" >\n",
    "\n",
    "To program this, its surprisingly simple now that we have a fast binary classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "\n",
    "    @property\n",
    "    def coef_(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return self.w_[:,1:]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    @property\n",
    "    def intercept_(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return self.w_[:,0]\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = (y==yval) # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            blr = VectorBinaryLogisticRegression(self.eta,\n",
    "                                                 self.iters)\n",
    "            blr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "            probs.append(blr.predict_proba(X)) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return self.unique_[np.argmax(self.predict_proba(X),axis=1)] # take argmax along row\n",
    "    \n",
    "lr = LogisticRegression(0.1,1500)\n",
    "print(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = load_iris()\n",
    "X = ds.data\n",
    "y = ds.target # note problem is NOT binary anymore, there are three classes!\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.8, test_size=0.2)\n",
    "\n",
    "\n",
    "lr = LogisticRegression(0.1,500)\n",
    "lr.fit(X_train,y_train)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"coef\\n\",lr.coef_)\n",
    "print(\"intercept\\n\",lr.intercept_)\n",
    "print(\"w\\n\",lr.w_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing to Scikit Learn\n",
    "Of course, there was no **need** to program the above except for the learning experience (or because we want to extend the learning algorithm without adjusting the scikit-learn source code). Now, let's use the builtin API for `sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "\n",
    "lr_sk = SKLogisticRegression(solver='liblinear') # all params default\n",
    "\n",
    "lr_sk.fit(X_train,y_train)\n",
    "print(np.hstack((lr_sk.intercept_[:,np.newaxis],lr_sk.coef_)))\n",
    "yhat = lr_sk.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this so much faster? The answer is parallelization is better handled through sklearn, separating out each classification, and the operations are ALL in lower level c++. **The main reason, however, is that  the update used is more advanced than what we have talked about.** They have an array of solvers that use the gradient (and Hessian) to solve the optimization more quickly. The accuracy is higher because this logistic regression object uses regularization, which helps control for over-learning, and also because the optimization allows for finer steps when we get to places where the gradient is small and the objective function is less well behaved. \n",
    "____\n",
    "# Logistic Regression Class Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear boundaries visualization from sklearn documentation\n",
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_decision_boundaries(lr,Xin,y,title=''):\n",
    "    Xb = copy.deepcopy(Xin)\n",
    "    lr.fit(Xb[:,:2],y) # train only on two features\n",
    "\n",
    "    h=0.01\n",
    "    # create a mesh to plot in\n",
    "    x_min, x_max = Xb[:, 0].min() - 1, Xb[:, 0].max() + 1\n",
    "    y_min, y_max = Xb[:, 1].min() - 1, Xb[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # get prediction values\n",
    "    Z = lr.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.5)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(Xb[:, 0], Xb[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "    plt.xlabel('Sepal length')\n",
    "    plt.ylabel('Sepal width')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "lr = LogisticRegression(0.1,1500) # this is still OUR LR implementation, not sklearn\n",
    "plot_decision_boundaries(lr,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 Regularized Logistic Regression\n",
    "Can we control these boundaries?\n",
    "Whenever we let the weights become large, that means we are allowing the sigmoid finction to also become more vertical (see image below). Large values of $w$ correspond to the blue line. Smaller values of $w$ correspond to the red line. So, in order to help prevent overfitting, we can add in a term into our optimization that incentivizes the weights to be smaller in magnitude to help prevent the sigmoid from having a large slope.\n",
    "\n",
    "<img src=\"https://www.mathworks.com/matlabcentral/mlc-downloads/downloads/submissions/51007/versions/2/screenshot.jpg\" alt=\"Drawing\" width= 300/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets as wd\n",
    "\n",
    "def sig_explorer(w):\n",
    "    x = np.linspace(-10,10,100)\n",
    "    y = expit(w*x)\n",
    "    plt.plot(x,y)\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"g(x)\")\n",
    "    plt.show()\n",
    "\n",
    "wd.interact(sig_explorer, w=(0.5,5,0.5),__manual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So keeping the weights small is identical to having smaller multipliers in the sigmoid function. Let's introduce another parameter in the objective function called the L2-Norm. The idea here is that we want to keep the values of the weights relatively small. This helps to control overfitting. We are changing our objective function by adding in a new summation of the weights that reduces the maximum value when weights are large:\n",
    "\n",
    "$$ l(\\mathbf{w})_{reg} = \\underbrace{l(\\mathbf{w})_{prev}}_{\\text{old objective}} - \\underbrace{C\\cdot\\sum_j w_j^2}_{\\text{regularization}} $$\n",
    "\n",
    "$C$ is a hyperparameter controlling how much we want to optimize the previous objective function versus keep the weights small. This means the gradient will be updated as follows:\n",
    "\n",
    "$$ \\underbrace{w_j}_{\\text{new value}} \\leftarrow \\underbrace{w_j}_{\\text{previous value}} + \\eta \\underbrace{\\left[\\left(\\sum_{i=1}^M (y^{(i)}-g(\\mathbf{w}^T\\mathbf{x}^{(i)}))x^{(i)}_j\\right) - C \\cdot 2w_j \\right]}_{\\text{new gradient}}$$\n",
    "\n",
    "Which is the same as:\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\left[\\underbrace{\\nabla l(\\mathbf{w})_{prev}}_{\\text{previous gradient}} - C \\cdot 2\\mathbf{w} \\right]$$\n",
    "\n",
    "Its fairly easy to implement because we just need to add to the existing gradient function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedBinaryLogisticRegression(VectorBinaryLogisticRegression):\n",
    "    # extend init functions\n",
    "    def __init__(self, C=0.0, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.C = C\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds) # call parent initializer\n",
    "        \n",
    "        \n",
    "    # extend previous class to change functionality\n",
    "    def _get_gradient(self,X,y):\n",
    "        # call get gradient from previous class\n",
    "        gradient = super()._get_gradient(X,y)\n",
    "        \n",
    "        # add in regularization (to all except bias term)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        return gradient\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now redefine the Logistic Regression Function where needed\n",
    "class RegularizedLogisticRegression(LogisticRegression):\n",
    "    def __init__(self, C=0.0, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.C = C\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds) # call parent initializer\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            # now this has regularization built into it\n",
    "            blr = RegularizedBinaryLogisticRegression(eta=self.eta,\n",
    "                                                      iterations=self.iters,\n",
    "                                                      C=self.C)\n",
    "            blr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets as wd\n",
    "\n",
    "# create a possible value of C from 0.001 to 0.1\n",
    "cost_vals = np.logspace(-3,1,15)\n",
    "def lr_explor(cost_idx):\n",
    "    C = cost_vals[cost_idx]\n",
    "    lr_clf = RegularizedLogisticRegression(eta=0.1,\n",
    "                                           iterations=2500, # lots of iterations (to help overfit)\n",
    "                                           C=C) # get object\n",
    "    \n",
    "    plot_decision_boundaries(lr_clf,X,y,title=\"C=%.5f\"%(C))\n",
    "    plt.show()\n",
    "\n",
    "wd.interact(lr_explor,cost_idx=(0,14,1),__manual=True)\n",
    "\n",
    "# what happens when C gets too large??\n",
    "# let's explore the different values and see what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Next Time: More Optimization with LR\n",
    "___\n",
    "\n",
    "In this notebook you learned:\n",
    "- Formulation of Logistic regression \n",
    "- Vectorizing Code to run Faster (in C++ via Numpy)\n",
    "- Regularization using L2-Norm of weights\n",
    "- Multi-Class Logistic Regression\n",
    "- LR Class Boundaries (showed they are linear) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "state": {
    "a5a3a566d7a049f68fa4ff56f9585df6": {
     "views": [
      {
       "cell_index": 35
      }
     ]
    },
    "cf13c251017e44d1b17bfc1c01d5b62e": {
     "views": [
      {
       "cell_index": 27
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
