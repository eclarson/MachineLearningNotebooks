{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Agenda\n",
    " - Numerical Optimization Techniques\n",
    "  - Types of Optimization\n",
    "  - Programming the Optimization\n",
    " - **Whirlwind Lecture Alert**\n",
    "  - Entire classes cover these concepts in expanded form\n",
    "  - But you are smart enough to get them in one lecture!\n",
    "    - Because science!\n",
    "    \n",
    "___\n",
    "\n",
    "# Last Time\n",
    "\n",
    "|Description| Equations, Derivations, Hessian Calculations, and Miscellaneous|\n",
    "|-----------|--------|\n",
    "| Sigmoid Definition | $$ p(y^{(i)}=1\\text{ | }\\mathbf{x}^{(i)},\\mathbf{w})=\\frac{1}{1+\\exp{(-\\mathbf{w}^T \\mathbf{x}^{(i)}})}$$ |\n",
    "| Log Likelihood | $$ l(\\mathbf{w}) = \\sum_i \\left( y^{(i)} \\ln [g(\\mathbf{w}^T \\mathbf{x}^{(i)})] + (1-y^{(i)}) (\\ln [1 - g(\\mathbf{w}^T \\mathbf{x}^{(i)})])  \\right)  $$ |\n",
    "| Vectorized Gradient | $$gradient =\\frac{1}{M}\\sum_{i=1}^M (y^{(i)}-g(\\mathbf{w}^T\\mathbf{x}^{(i)}))\\mathbf{x}^{(i)}$$ |\n",
    "| Regularization | $$ \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\left[\\underbrace{\\nabla l(\\mathbf{w})_{old}}_{\\text{old gradient}} - C \\cdot 2\\mathbf{w} \\right]$$|\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"PDF_slides/mark_scooter.png\"  width=\"300\">\n",
    "\n",
    "\n",
    "# More Advanced  Optimization for Machine Learning\n",
    "From previous notebooks, we know that the logistic regression update equation is given by:\n",
    "\n",
    "$$ \\underbrace{w_j}_{\\text{new value}} \\leftarrow \\underbrace{w_j}_{\\text{old value}} + \\eta \\underbrace{\\left[\\left(\\sum_{i=1}^M (y^{(i)}-g(\\mathbf{w}^T\\mathbf{x}^{(i)}))x^{(i)}_j\\right) - C \\cdot 2w_j \\right]}_{\\nabla l(w)}$$\n",
    "\n",
    "Which can be made into more generic notation by denoting the objective function as $l(\\mathbf{w})$ and the gradient calculation as $\\nabla$:\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\nabla l(\\mathbf{w})$$\n",
    "\n",
    "<img src=\"PDF_slides/batch.gif\"  width=\"400\">\n",
    "\n",
    "One problem is that we still need to set the value of $\\eta$, which can drastically change the performance of the optimization algorithm. If $\\eta$ is too large, the algorithm might be unstable. If $\\eta$ is too small, it might take a long time (i.e., many iterations) to converge.\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} + \\underbrace{\\eta}_{\\text{best step?}} \\nabla l(\\mathbf{w}) $$\n",
    "\n",
    "We can solve this issue by performing a line search for the best value of $\\eta$ along the direction of the gradient, as denoted by:\n",
    "\n",
    "$$ \\eta \\leftarrow \\arg\\max_\\eta \\underbrace{\\sum_{i=1}^M (y^{(i)}-g(\\mathbf{w}^T\\mathbf{x}^{(i)}))^2 -C\\cdot\\sum_j w_j^2}_{\\nabla l(\\mathbf{w})} $$\n",
    "\n",
    "<img src=\"PDF_slides/line_search.gif\"  width=\"400\">\n",
    "\n",
    "## Optimizing Logistic Regression via Line Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit\n",
    "\n",
    "ds = load_iris()\n",
    "X = ds.data\n",
    "y = (ds.target>1).astype(np.int) # make problem binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-0.19960106]\n",
      " [-0.45578023]\n",
      " [-0.54503119]\n",
      " [ 0.77831275]\n",
      " [ 0.51499827]]\n",
      "Accuracy of:  0.9733333333333334\n",
      "CPU times: user 5.31 ms, sys: 2.52 ms, total: 7.83 ms\n",
      "Wall time: 5.68 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# from last time, our logistic regression algorithm is given by (including everything we previously had):\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "            # add bacause maximizing \n",
    "\n",
    "blr = BinaryLogisticRegression(eta=0.1,iterations=50,C=0.001)\n",
    "\n",
    "blr.fit(X,y)\n",
    "print(blr)\n",
    "\n",
    "yhat = blr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-1.96717802]\n",
      " [-4.81050143]\n",
      " [-6.74146326]\n",
      " [ 8.75170244]\n",
      " [ 5.12131168]]\n",
      "Accuracy of:  0.98\n",
      "CPU times: user 7.03 ms, sys: 2.23 ms, total: 9.27 ms\n",
      "Wall time: 7.42 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericlarson/opt/anaconda3/envs/mlenv2021/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# and we can update this to use a line search along the gradient like this:\n",
    "from scipy.optimize import minimize_scalar\n",
    "import copy\n",
    "from numpy import ma # (masked array) this has most numpy functions that work with NaN data.\n",
    "class LineSearchLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    # define custom line search for problem\n",
    "    def __init__(self, line_iters=0.0, **kwds):        \n",
    "        self.line_iters = line_iters\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds) # call parent initializer\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_function(eta,X,y,w,grad,C):\n",
    "        wnew = w - grad*eta\n",
    "        g = expit(X @ wnew)\n",
    "        return -np.sum(ma.log(g[y==1]))-ma.sum(np.log(1-g[y==0])) + C*sum(wnew**2)\n",
    "    \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = -self._get_gradient(Xb,y)\n",
    "            # minimization inopposite direction\n",
    "            \n",
    "            # do line search in gradient direction, using scipy function\n",
    "            opts = {'maxiter':self.line_iters} # unclear exactly what this should be\n",
    "            res = minimize_scalar(self.objective_function, # objective function to optimize\n",
    "                                  bounds=(0,self.eta*10), #bounds to optimize\n",
    "                                  args=(Xb,y,self.w_,gradient,self.C), # additional argument for objective function\n",
    "                                  method='bounded', # bounded optimization for speed\n",
    "                                  options=opts) # set max iterations\n",
    "            \n",
    "            eta = res.x # get optimal learning rate\n",
    "            self.w_ -= gradient*eta # set new function values\n",
    "            # subtract to minimize\n",
    "                \n",
    "            \n",
    "\n",
    "lslr = LineSearchLogisticRegression(eta=1,\n",
    "                                    iterations=5, \n",
    "                                    line_iters=5, \n",
    "                                    C=0.001)\n",
    "\n",
    "lslr.fit(X,y)\n",
    "\n",
    "yhat = lslr.predict(X)\n",
    "print(lslr)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performs well, but was not too much faster than previously (this is because $\\eta$ was chosen well in the initial example). \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Test\n",
    "How much computation (i.e., how many multiplies) are required for calculating the gradient of:\n",
    "$$ \\left( \\frac{1}{M}\\left[\\sum_{i=1}^M (y^{(i)}-g(\\mathbf{w}^T\\mathbf{x}^{(i)}))\\mathbf{x}^{(i)}\\right] - 2C\\cdot \\mathbf{w}\\right) $$\n",
    "\n",
    "Where $M$ is the number of instance and $N$ is the number of elements in $\\mathbf{w}$.\n",
    "\n",
    "- A: $ M\\cdot N+1$\n",
    "- B: $ (M+1)\\cdot N$\n",
    "- C: $ 2N $ \n",
    "- D: $ 2N-M$ \n",
    "_____\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "Sometimes the gradient calcualtion is too computational:\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta\\left( \\frac{1}{M}\\left[\\sum_{i=1}^M (y^{(i)}-g(\\mathbf{w}^T\\mathbf{x}^{(i)}))\\mathbf{x}^{(i)}\\right] - 2C\\cdot \\mathbf{w}\\right) $$\n",
    "\n",
    "Instead, we can approximate the gradient using one instance, this is called stochastic gradient descent (SGD) because the steps can appear somewhat random.\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\underbrace{\\left((y^{(i)}-g(\\mathbf{w}^T\\mathbf{x}^{(i)}))\\mathbf{x}^{(i)}-2C\\cdot \\mathbf{w}\\right)}_{\\text{approx. gradient}} \\text{,   where   } i\\in M$$\n",
    "\n",
    "<img src=\"PDF_slides/SGD.gif\"  width=\"400\">\n",
    "\n",
    "Let's code up the SGD example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-1.06918175]\n",
      " [-1.92544775]\n",
      " [-1.69146291]\n",
      " [ 2.93323941]\n",
      " [ 2.67448382]]\n",
      "Accuracy of:  0.9466666666666667\n",
      "CPU times: user 26.9 ms, sys: 1.49 ms, total: 28.4 ms\n",
      "Wall time: 28.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class StochasticLogisticRegression(BinaryLogisticRegression):\n",
    "    # stochastic gradient calculation \n",
    "    def _get_gradient(self,X,y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    \n",
    "slr = StochasticLogisticRegression(eta=0.05, iterations=1500, C=0.001) # take a lot more steps!!\n",
    "\n",
    "slr.fit(X,y)\n",
    "\n",
    "yhat = slr.predict(X)\n",
    "print(slr)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<img src=\"PDF_slides/BtJXjJcCAAE7QOB.jpg\"  width=\"300\">\n",
    "\n",
    "\n",
    "\n",
    "# Optimizing with Second Order Derivatives\n",
    "First, let's look at the one dimensional case when we have a function $l(w)$ where w is a scalar. The optimal value of w is given by:\n",
    "\n",
    "$$ w \\leftarrow w - \\underbrace{[\\frac{\\partial^2}{\\partial w}l(w)]^{-1}}_{\\text{inverse 2nd deriv}}\\underbrace{\\frac{\\partial}{\\partial w}l(w)}_{\\text{derivative}}  $$\n",
    "\n",
    "Note that if $l(w)$ is a quadrativ function, this solution converges in a single step!\n",
    "\n",
    "\n",
    "|Aside: an example with the second derivative:|\n",
    "|------------------------------------------------------------------------|\n",
    "|Say $l(w)=2w^2+4w+5$, and we want to minimize the function. We have that:\n",
    "|$$\\frac{\\partial}{\\partial w}l(w)=4w+4$$|\n",
    "| $$\\frac{\\partial^2}{\\partial w}l(w)=4$$|\n",
    "|Therefore, if we choose $w_{start}=0$, we have:|\n",
    "|$$\\frac{\\partial}{\\partial w}l(0)=4$$  |\n",
    "|$$\\frac{\\partial^2}{\\partial w}l(0)=4$$ |\n",
    "|So the update becomes|\n",
    "|$$w \\leftarrow w_{start} - \\frac{1}{4}4 = -1$$|\n",
    "|The solution is found in one step. This works for any initial value of $w_{start}$. Let's verify that the solution worked graphically.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(-1, 2.5, '$\\\\leftarrow$found minimum')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "w = np.linspace(-7,5,100)\n",
    "l = 2*w**2+4*w+5\n",
    "plt.plot(w,l)\n",
    "plt.text(-1,2.5,'$\\leftarrow$found minimum',fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Newton's Update Method\n",
    "<img src=\"PDF_slides/newton.png\"  width=\"600\">\n",
    "\n",
    "But how do we translate this over to objective funtions with more than one variable? We need a second derivative of a multivariate equation... enter, the hessian. Our new update is defined by Newton's method:\n",
    "\n",
    "$$ w \\leftarrow w - \\underbrace{[\\frac{\\partial^2}{\\partial w}l(w)]^{-1}}_{\\text{inverse 2nd deriv}}\\underbrace{\\frac{\\partial}{\\partial w}l(w)}_{\\text{derivative}}  $$\n",
    "\n",
    "such that, in multiple dimensions we can approximate the update as:\n",
    "\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\cdot \\underbrace{\\mathbf{H}(\\mathbf{w})^{-1}}_{\\text{inverse Hessian}}\\cdot\\underbrace{\\nabla l(\\mathbf{w})}_{\\text{gradient}}$$\n",
    "\n",
    "where the Hessian is defined as follows for any multivariate equation $l(\\mathbf{w})$:\n",
    "$$ \\nabla^2 l(\\mathbf{w}) = \\mathbf{H}(\\mathbf{w})   $$\n",
    "\n",
    "$$  \\mathbf{H}(\\mathbf{w}) =  \\begin{bmatrix}\n",
    "        \\frac{\\partial^2}{\\partial w_1}l(\\mathbf{w}) &  \\frac{\\partial}{\\partial w_1}\\frac{\\partial}{\\partial w_2}l(\\mathbf{w}) & \\ldots     & \\frac{\\partial}{\\partial w_1}\\frac{\\partial}{\\partial w_N}l(\\mathbf{w})  \\\\\n",
    "        \\frac{\\partial}{\\partial w_2}\\frac{\\partial}{\\partial w_1}l(\\mathbf{w})  & \\frac{\\partial^2}{\\partial w_2}l(\\mathbf{w}) &  \\ldots     & \\frac{\\partial}{\\partial w_2}\\frac{\\partial}{\\partial w_N}l(\\mathbf{w})  \\\\\n",
    "        &  \\vdots &\\\\\n",
    "        \\frac{\\partial}{\\partial w_N}\\frac{\\partial}{\\partial w_1}l(\\mathbf{w})  & \\frac{\\partial}{\\partial w_N}\\frac{\\partial}{\\partial w_2}l(\\mathbf{w}) &  \\ldots     & \\frac{\\partial^2}{\\partial w_N}l(\\mathbf{w}) \\\\\n",
    "     \\end{bmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "For logistic regression, we can calculate the formula for the $j^{th}$ and $k^{th}$ element of the Hessian as follows:\n",
    "\n",
    "$$ \\mathbf{H}_{j,k}(\\mathbf{w}) = \\frac{\\partial}{\\partial w_k} \\underbrace{\\frac{\\partial}{\\partial w_j}l(\\mathbf{w})}_{\\text{first derivative}} $$\n",
    "\n",
    "But we already know the result of the $j^{th}$ partial derivative from our calculation of $\\nabla l(\\mathbf{w})$: \n",
    "\n",
    "$$ \\frac{\\partial}{\\partial w_j}l(\\mathbf{w}) = \\sum_i \\left(y^{(i)}-g(\\mathbf{w}^T\\cdot\\mathbf{x}^{(i)})\\right)x_j^{(i)} $$\n",
    "\n",
    "So we can plug this back into the equation to get:\n",
    "\n",
    "$$ \n",
    "\\begin{split}\n",
    "\\mathbf{H}_{j,k}(\\mathbf{w}) & = \\frac{\\partial}{\\partial w_k}\\sum_i \\left(y^{(i)}-g(\\mathbf{w}^T\\cdot\\mathbf{x}^{(i)})\\right)x_j^{(i)} \\\\\n",
    " & = \\underbrace{\\sum_i \\frac{\\partial}{\\partial w_k} y^{(i)}x_j^{(i)}}_{\\text{no dependence on }k\\text{, zero}} -\\sum_i \\frac{\\partial}{\\partial w_k}g(\\mathbf{w}^T\\cdot\\mathbf{x}^{(i)})x_j^{(i)} \\\\\n",
    " & = -\\sum_i x_j^{(i)}\\underbrace{\\frac{\\partial}{\\partial w_k}g(\\mathbf{w}^T\\cdot\\mathbf{x}^{(i)})}_{\\text{already know this as }g(1-g)x_k} \\\\\n",
    " & =  -\\sum_{i=1}^M \\left[g(\\mathbf{w}^T\\mathbf{x}^{(i)})[1-g(\\mathbf{w}^T\\mathbf{x}^{(i)})]\\right]\\cdot{x_k}^{(i)}{x_j}^{(i)} \\\\\n",
    "\\end{split}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Therefore the Hessian for logistic regression becomes (adding in the regularization term also):\n",
    "$$ \\mathbf{H}_{j,k}(\\mathbf{w}) =\\left( -\\sum_{i=1}^M \\underbrace{\\left[g(\\mathbf{w}^T\\mathbf{x}^{(i)})[1-g(\\mathbf{w}^T\\mathbf{x}^{(i)})]\\right]}_{\\text{scalar value for each instance}}\\cdot\\underbrace{{x_k}^{(i)}{x_j}^{(i)}}_{i^{th}\\text{ instance elements}} \\right) + \\underbrace{2\\cdot C}_{\\text{regularization}}  $$\n",
    "\n",
    "\n",
    "This equation can be calcuated in a for loop, for each $j,k$ element in the Hessian and for each instance in the dataset, but this would be **slow** in python. To vectorize this operation, we need to have each operation be linear algebra, so that it can be runn efficiently with numpy. \n",
    "\n",
    "First notice that the sum of each terms ${x_k}^{(i)}{x_j}^{(i)}$ that forms a matrix can be calculated as follows:\n",
    "\n",
    "$$    \n",
    "\\begin{bmatrix}\n",
    "        \\sum_{i=1}^M {x_1}^{(i)}{x_1}^{(i)} & \\sum_{i=1}^M {x_1}^{(i)}{x_2}^{(i)} & \\ldots     &  \\sum_{i=1}^M {x_1}^{(i)}{x_N}^{(i)} \\\\\n",
    "        \\sum_{i=1}^M {x_2}^{(i)}{x_1}^{(i)} & \\sum_{i=1}^M {x_2}^{(i)}{x_2}^{(i)} & \\ldots     &  \\sum_{i=1}^M {x_2}^{(i)}{x_N}^{(i)} \\\\\n",
    "        &  \\vdots & \\\\\n",
    "        \\sum_{i=1}^M {x_N}^{(i)}{x_1}^{(i)} & \\sum_{i=1}^M {x_N}^{(i)}{x_2}^{(i)} & \\ldots     &  \\sum_{i=1}^M {x_N}^{(i)}{x_N}^{(i)} \\\\ \\\\\n",
    "\\end{bmatrix}   \n",
    "%\n",
    "= \\mathbf{X}^T \\cdot\\mathbf{I} \\cdot\\mathbf{X}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{I}$ is the identity matrix of size $M\\text{x}M$. This can be seen in the following exploded view of the matrix operations: \n",
    "\n",
    "$$ \\mathbf{X}^T \\cdot\\mathbf{I} \\cdot\\mathbf{X}=\n",
    "\\begin{bmatrix}\n",
    "        \\uparrow &    \\uparrow    &    & \\uparrow  \\\\\n",
    "        \\mathbf{x}^{(1)} &  \\mathbf{x}^{(2)} & \\ldots   & \\mathbf{x}^{(M)}  \\\\\n",
    "        \\downarrow &    \\downarrow    &   & \\downarrow  \\\\\n",
    "\\end{bmatrix}\n",
    "%\n",
    "\\begin{bmatrix}\n",
    "        1 &  0 & \\ldots & 0 \\\\\n",
    "        0 &  1 & \\ldots & 0 \\\\\n",
    "          &  \\vdots  & &    \\\\\n",
    "        0 &  0 & \\ldots & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "%\n",
    "\\begin{bmatrix}\n",
    "        \\leftarrow &  \\mathbf{x}^{(1)}      & \\rightarrow  \\\\\n",
    "        \\leftarrow &  \\mathbf{x}^{(2)}      & \\rightarrow  \\\\\n",
    "        &  \\vdots &\\\\\n",
    "        \\leftarrow &  \\mathbf{x}^{(M)}      & \\rightarrow  \\\\\n",
    "\\end{bmatrix}\n",
    "%\n",
    "$$\n",
    "\n",
    "With this equation in mind, we can revisit the calcualtion of the Hessian and use matrix operations to define the needed multiplication in an exploded view of the operations:\n",
    "\n",
    "$$ \\mathbf{H}[l(\\mathbf{w})]=\n",
    "\\begin{bmatrix}\n",
    "        \\uparrow &    \\uparrow    &    & \\uparrow  \\\\\n",
    "        \\mathbf{x}^{(1)} &  \\mathbf{x}^{(2)} & \\ldots   & \\mathbf{x}^{(M)}  \\\\\n",
    "        \\downarrow &    \\downarrow    &   & \\downarrow  \\\\\n",
    "\\end{bmatrix}\n",
    "%\n",
    "\\begin{bmatrix}\n",
    "        g(\\mathbf{w}^T\\mathbf{x}^{(1)})[1-g(\\mathbf{w}^T\\mathbf{x}^{(1)})] & \\ldots & 0 \\\\\n",
    "          &  \\vdots   &    \\\\\n",
    "        0  & \\ldots & g(\\mathbf{w}^T\\mathbf{x}^{(M)})[1-g(\\mathbf{w}^T\\mathbf{x}^{(M)})] \\\\\n",
    "\\end{bmatrix}\n",
    "%\n",
    "\\begin{bmatrix}\n",
    "        \\leftarrow &  \\mathbf{x}^{(1)}      & \\rightarrow  \\\\\n",
    "        \\leftarrow &  \\mathbf{x}^{(2)}      & \\rightarrow  \\\\\n",
    "        &  \\vdots &\\\\\n",
    "        \\leftarrow &  \\mathbf{x}^{(M)}      & \\rightarrow  \\\\\n",
    "\\end{bmatrix}\n",
    "%\n",
    "$$\n",
    "\n",
    "___\n",
    "Or, more succintly as follows (adding in the regularization term as well):\n",
    "\n",
    "$$ \\mathbf{H}[l(\\mathbf{w})] =  \\mathbf{X}^T \\cdot \\text{diag}\\left[g(\\mathbf{X}\\cdot\\mathbf{w})\\odot(1-g(\\mathbf{X}\\cdot\\mathbf{w}))\\right]\\cdot \\mathbf{X} -2C$$\n",
    "\n",
    "___\n",
    "\n",
    "Now we can place the Hessian derivation into the Newton Update Equation, like this:\n",
    "\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\cdot \\underbrace{\\mathbf{H}[l(\\mathbf{w})]^{-1}}_{\\text{inverse Hessian}}\\cdot\\underbrace{\\nabla l(\\mathbf{w})}_{\\text{gradient}}$$\n",
    "\n",
    "Adding in the exact equations for the Hessian and gradient, we can finally get:\n",
    "\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\cdot \\underbrace{\\left[\\mathbf{X}^T \\cdot \\text{diag}\\left[g(\\mathbf{X}\\cdot\\mathbf{w})\\odot(1-g(\\mathbf{X}\\cdot\\mathbf{w}))\\right] \\cdot \\mathbf{X} -2C \\right]^{-1} }_{\\text{inverse Hessian}} \\cdot \\underbrace{\\mathbf{X}\\odot y_{diff}}_{\\text{gradient}}$$\n",
    "\n",
    "\n",
    "You can see the full derivation of the Hessian in my hand written notes here also:\n",
    "- https://raw.githubusercontent.com/eclarson/MachineLearningNotebooks/master/PDF_Slides/HessianCalculation.pdf\n",
    "\n",
    "\n",
    "\n",
    "So let's code this up using numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-10.33521361]\n",
      " [ -1.11842524]\n",
      " [ -0.99722044]\n",
      " [  2.33435715]\n",
      " [  5.32207095]]\n",
      "Accuracy of:  0.98\n",
      "CPU times: user 10.4 ms, sys: 2.13 ms, total: 12.6 ms\n",
      "Wall time: 3.58 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from numpy.linalg import pinv\n",
    "class HessianBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # just overwrite gradient function\n",
    "    def _get_gradient(self,X,y):\n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X - 2 * self.C # calculate the hessian\n",
    "\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return pinv(hessian) @ gradient\n",
    "       \n",
    "hlr = HessianBinaryLogisticRegression(eta=1.0,\n",
    "                                      iterations=4,\n",
    "                                      C=0.001) # note that we need only a few iterations here\n",
    "\n",
    "hlr.fit(X,y)\n",
    "yhat = hlr.predict(X)\n",
    "print(hlr)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Can we still do better? Problems With the Hessian:\n",
    "Quadratic isn’t always a great assumption:\n",
    " - highly dependent on starting point\n",
    "  - jumps can get really random!\n",
    " - near saddle points, inverse hessian unstable\n",
    " - hessian not always invertible… or invertible with correct numerical precision\n",
    " \n",
    "The Hessian can sometimes be ill formed for these problems and can also be highly computational. Thus, we prefer to approximate the Hessian, and approximate its inverse to better control the steps we make and directions we use.  \n",
    "\n",
    "# Quasi-Newton Methods\n",
    "In general:\n",
    " - Approximate the Hessian with something numerically sound and efficiently invertible \n",
    " - Back off to gradient descent when the approximate hessian is not stable\n",
    " - Try to create an approximation with as many properties of the Hessian as possible, like being symmetric and positive semi-definite\n",
    " - A popular approach: Rank One Hessian Approximation\n",
    " - An even more popular appraoch: Rank Two, with Broyden-Fletcher-Goldfarb-Shanno (BFGS)\n",
    "\n",
    "\n",
    "### Rank One Hessian Approximation \n",
    "Let's work our way up to using BFGS by first looking at one quasi-newton method, the rank one Hessian approximation. **Note, I only want you to get an intuition for this process. There is no requirenemtn to understand the derivation completely.**\n",
    "Essentially, we want to update the Hessian with an approximation that is easily invertible and based on stable gradient calculations. We can define the approximate Hessian for each iteration, $\\mathbf{H}_k$. To start as simple as possible, we will assume the Hessian can be approximated with one vector. Let's start off with a few other assumptions.\n",
    "\n",
    "___\n",
    "One property of the hessian is called the Secant equation, which relates the change in input to the change in the derivative. The Secant Equation (exact for quadratic functions) is:\n",
    "$$ \\mathbf{H}_k \\underbrace{(\\mathbf{w}_{k+1} - \\mathbf{w}_k)}_{\\text{Change in }w} = \\underbrace{\\nabla l(\\mathbf{w}_{k+1}) - \\nabla l(\\mathbf{w}_k)}_{\\text{Change in gradient}}$$\n",
    "\n",
    "or, using intermediate variables for the differences: \n",
    "$$ \\mathbf{H}_{k+1} \\mathbf{s}_k =  \\mathbf{v}_k $$\n",
    "\n",
    "where $ \\mathbf{s} = (\\mathbf{w}_{k+1} - \\mathbf{w}_k) $ and $ \\mathbf{v} = (\\nabla l(\\mathbf{w}_{k+1}) - \\nabla l(\\mathbf{w}_k)) $, the difference in the gradient. If we enforce this relationship, we can find the hessian, assuming that the current location is approximated well by a quadratic (making the Secant a good assumption also). We also want the Hessian to be symmetric and not too far away from its initial value (for stable optimization).\n",
    "___\n",
    "\n",
    "For optimizing, we would like to be able to iteratively update the Hessian from some original guess,$\\mathbf{H}_k$, and have the update be easy to calculate. Therefore, we can choose the update of the Hessian to be approximated by the rank one update (one vector). Since the Hessian is the second partial derivative, a starting approximation might be the gradient difference we already defined such that  $\\mathbf{H} \\approx \\mathbf{v}\\cdot\\mathbf{v}^T$ which would form a matrix of the differences of each partial deriviative in the gradient. In practice, we need a vector that is slightly less constrained, such that    \n",
    "$$ \\mathbf{H}_{k+1}=\\mathbf{H}_k+\\alpha_k\\mathbf{u}\\cdot\\mathbf{u}^T $$\n",
    "Where $\\mathbf{u}$ and $\\alpha_k$ can be anything we want. \n",
    "\n",
    "Substituting back into the secant formula:\n",
    "$$ (\\mathbf{H}_k +\\alpha_k\\mathbf{u}_k\\cdot\\mathbf{u}_k^T)\\mathbf{s}_{k} = \\mathbf{v}_{k} $$\n",
    "\n",
    "\n",
    "One solution of this equation (there are infinite solutions) is to use one that simplifies nicely:\n",
    "$$ \\mathbf{u}_k=\\mathbf{v}_{k}-\\mathbf{H}_k \\mathbf{s}_{k} \\text{   and   } \\alpha_k=\\frac{1}{(\\mathbf{v}_{k}-\\mathbf{H}_k \\mathbf{s}_{k})\\mathbf{s}_{k}}=\\frac{1}{\\mathbf{u}_{k}^T\\mathbf{s}_{k}}$$\n",
    "\n",
    "___\n",
    "and combining this with our initial $\\mathbf{H}_{k+1}$ formula:\n",
    "$$ \\mathbf{H}_{k+1}=\\mathbf{H}_k- \\frac{\\mathbf{u}_k\\mathbf{u}_k^T}{\\mathbf{u}_k^T\\mathbf{s}_{k}} $$\n",
    "\n",
    "This gives an update for the Hessian, which we can use in our optimization formula. However, we need to define the vectors using the secant equation assumptions, such that $ \\mathbf{v}_k $ and $ \\mathbf{s}_k $ are the difference in gradients and weights as defined, respectively.\n",
    "\n",
    "We can also assume that the inverse of the Hessian can be optimized and formulate similar equations for its update, based upon the previous inverse. Therefore, we need the inverse of $(\\mathbf{H}_k+\\mathbf{v}\\cdot\\mathbf{v}^T)^{-1}$, which luckily has a closed form solution according to the Sherman-Morrison formula:\n",
    "\n",
    "$$ (\\mathbf{A}+\\mathbf{v}\\cdot\\mathbf{v}^T)^{-1} = \\mathbf{A}^{-1} - \\frac{\\mathbf{A}^{-1} \\mathbf{v} \\mathbf{v}^T\\mathbf{A}^{-1}}{1+\\mathbf{v}^T \\mathbf{A}^{-1} \\mathbf{v}}  $$\n",
    "___\n",
    "\n",
    "Now the optimization can be described as a rank one approximation of the Hessian. Placing it all together, we can get the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|Description| Equations, Derivations, Hessian Calculations, and Miscellaneous |\n",
    "|-----------|--------|\n",
    "| **Definitions with Rank 1 Approximation** |  |\n",
    "|$$ \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\cdot \\underbrace{\\mathbf{H}[l(\\mathbf{w})]^{-1}}_{\\text{inverse Hessian}}\\cdot\\underbrace{\\nabla l(\\mathbf{w})}_{\\text{gradient}}$$ |  |\n",
    "|1. Initial Approx. Hessian for $k=0$ is identity matrix| $$\\mathbf{H}_0=\\mathbf{I}$$|\n",
    "|2. Find update direction, $\\mathbf{p}_k$ | $$ \\mathbf{p}_k = -\\mathbf{H}_k^{-1} \\nabla l(\\mathbf{w}_k) $$| \n",
    "|3. Update $\\mathbf{w}$|$$\\mathbf{w}_{k+1}\\leftarrow \\mathbf{w}_k + \\eta \\cdot \\mathbf{p}_k $$|\n",
    "|4. Save scaled direction ($\\mathbf{w}_{k+1}-\\mathbf{w}_k$)| $$\\mathbf{s}_k=\\eta \\cdot \\mathbf{p}_k$$ |\n",
    "|5a. Approximate change in derivative | $$\\mathbf{v}_k = \\nabla l(\\mathbf{w}_{k+1}) - \\nabla l(\\mathbf{w}_k) $$|\n",
    "| 5b. Define $\\mathbf{u}$ from above: | $$\\mathbf{u}_k=\\mathbf{v}_k-\\mathbf{H}_k\\mathbf{s}_k$$| \n",
    "|6. Redefine approx Hessian update| $$\\mathbf{H}_{k+1}=\\mathbf{H}_k+\\underbrace{\\frac{\\mathbf{u}_k \\mathbf{u}_k^T}{\\mathbf{u}_k^T \\mathbf{s}_k}}_{\\text{approx. Hessian}} $$ |\n",
    "|7. Approx. Inverse $\\mathbf{H}_{k+1}^{-1}$ via Sherman Morris| $$ \\mathbf{H}_{k+1}^{-1} = \\mathbf{H}_{k}^{-1} - \\frac{\\mathbf{H}_k^{-1} \\mathbf{u}_k \\mathbf{u}_k^T\\mathbf{H}_k^{-1}}{1+\\mathbf{u}_k^T \\mathbf{H}_k^{-1} \\mathbf{u}_k} $$ |\n",
    "| 8. Repeat starting at step 2| $$ k = k+1 $$| \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### BFGS\n",
    "Although the rank one approximation is a good performer, it can be improved by adding some additional criteria to the Hessian approximation. In this case, we assume that the $\\mathbf{H}_k$ needs to also be positive semi-definite, which helps with numerical stability. One of the most popular quasi-Newton methods that does this is known as Broyden-Fletcher-Goldfarb-Shanno (BFGS). \n",
    "- https://en.wikipedia.org/wiki/Broyden–Fletcher–Goldfarb–Shanno_algorithm \n",
    "\n",
    "In this formulation we add an additional vector matrix addition to the update equation that ensure the resulting matrix is positive semidefinite:\n",
    "$$ \\mathbf{H}_{k+1}=\\mathbf{H}_k+\\alpha_k\\mathbf{u}_k\\cdot\\mathbf{u}_k^T + \\beta_k\\mathbf{z}_k\\cdot\\mathbf{z}_k^T$$\n",
    "\n",
    "The derivation is intuitively similar to the previous rank one apprximation. However, it becomes easier to obtain simple values for $\\mathbf{u}$ and $\\mathbf{z}$ as follows:\n",
    "$$ \\mathbf{u}_k = \\mathbf{v}_k   \\text{    and    } \\mathbf{z}_k=\\mathbf{H}_k \\mathbf{s}_k  $$\n",
    "\n",
    "After solving for the $\\alpha_k$ and $\\beta_k$ coefficients, we get the update equation as:\n",
    "$$  \\mathbf{H}_{k+1}=\\mathbf{H}_k+\\underbrace{\\frac{\\mathbf{v}_k \\mathbf{v}_k^T}{\\mathbf{v}_k^T \\mathbf{s}_k}}_{\\text{previous}} -\\underbrace{\\frac{\\mathbf{H}_k \\mathbf{s}_k \\mathbf{s}_k^T \\mathbf{H}_k}{\\mathbf{s}_k^T \\mathbf{H}_k \\mathbf{s}_k}}_{\\text{new}}  $$\n",
    "\n",
    "The complete formulation can replace steps from the previous rank update as follows:\n",
    "\n",
    "|Description| Equations, Derivations, Hessian Calculations, and Miscellaneous |\n",
    "|-----------|--------|\n",
    "| **Alternative Definitions with Rank 2 (BFGS)** |  |\n",
    "| 6. Redefine approx Hessian| $$\\mathbf{H}_{k+1}=\\mathbf{H}_k+\\frac{\\mathbf{v}_k \\mathbf{v}_k^T}{\\mathbf{v}_k^T \\mathbf{s}_k} -\\frac{\\mathbf{H}_k \\mathbf{s}_k \\mathbf{s}_k^T \\mathbf{H}_k}{\\mathbf{s}_k^T \\mathbf{H}_k \\mathbf{s}_k} $$ |\n",
    "|7. Approximate Inverse $\\mathbf{H}_{k+1}^{-1}$ via Sherman Morris| $$ \\mathbf{H}_{k+1}^{-1} = \\mathbf{H}_{k}^{-1} + \\frac{(\\mathbf{s}_k^T \\mathbf{v}_k+\\mathbf{H}_{k}^{-1})(\\mathbf{s}_k \\mathbf{s}_k^T)}{(\\mathbf{s}_k^T \\mathbf{v}_k)^2}-\\frac{\\mathbf{H}_{k}^{-1} \\mathbf{v}_k \\mathbf{s}_k^T+\\mathbf{s}_k \\mathbf{v}_k^T\\mathbf{H}_{k}^{-1}}{\\mathbf{s}_k^T \\mathbf{v}_k} $$|\n",
    "\n",
    "___\n",
    "We won't explicitly program the BFGS algorithm--instead we can take advantage of scipy's calculations to do it for us. For using this algorithm, we need to define the objective function and the gradient explicitly for another program to calculate. \n",
    "Recall that Logistic regression uses the following objective function:\n",
    "\n",
    "$$ l(w) = \\left(\\sum_i y^{(i)} \\ln g(\\mathbf{x}^{(i)}) + (1-y^{(i)})\\ln[1-g(\\mathbf{x}^{(i)})]\\right)  - C \\cdot \\sum_j w_j^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-0.11105493]\n",
      " [-0.26703712]\n",
      " [-0.37959666]\n",
      " [ 0.4941737 ]\n",
      " [ 0.28604208]]\n",
      "Accuracy of:  0.98\n",
      "CPU times: user 5.66 ms, sys: 2.09 ms, total: 7.74 ms\n",
      "Wall time: 6.24 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# for this, we won't perform our own BFGS implementation \n",
    "# (it takes a fair amount of code and understanding, which we haven't setup yet)\n",
    "# luckily for us, scipy has its own BFGS implementation:\n",
    "from scipy.optimize import fmin_bfgs\n",
    "from numpy import ma\n",
    "class BFGSBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_function(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        # invert this because scipy minimizes, but we derived all formulas for maximzing\n",
    "        return -np.sum(ma.log(g[y==1]))-np.sum(ma.log(1-g[y==0])) + C*sum(w**2) \n",
    "        #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_gradient(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        gradient[1:] += -2 * w[1:] * C\n",
    "        return -gradient\n",
    "    \n",
    "    # just overwrite fit function\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = fmin_bfgs(self.objective_function, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient, # gradient function\n",
    "                            args=(Xb,y,self.C), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False)\n",
    "        \n",
    "        self.w_ = self.w_.reshape((num_features,1))\n",
    "            \n",
    "bfgslr = BFGSBinaryLogisticRegression(_,iterations=2,C=0.001) # note that we need only a few iterations here\n",
    "\n",
    "bfgslr.fit(X,y)\n",
    "yhat = bfgslr.predict(X)\n",
    "print(bfgslr)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#str(bfgslr.eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BFGS for Multiclass Logistic Regression\n",
    "Now let's add BFGS to non-binary classification. As before, we will use one-versus-all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, \n",
    "                 C=0.0001, \n",
    "                 solver=BFGSBinaryLogisticRegression):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.solver = solver\n",
    "        self.classifiers_ = []\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = np.array(y==yval).astype(int) # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            \n",
    "            hblr = self.solver(eta=self.eta,iterations=self.iters,C=self.C)\n",
    "            hblr.fit(X,y_binary)\n",
    "\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(hblr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for hblr in self.classifiers_:\n",
    "            probs.append(hblr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "ds = load_iris()\n",
    "X = ds.data\n",
    "\n",
    "X = StandardScaler().fit(X).transform(X)\n",
    "y_not_binary = ds.target # note problem is NOT binary anymore, there are three classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[-2.59625724 -1.28763064  2.21883671 -2.35044198 -2.18813616]\n",
      " [-0.98695377  0.40935313 -1.45826406  0.51669765 -0.95325866]\n",
      " [-6.35603369  0.15248126 -1.08851042  3.86625795  5.59506246]]\n",
      "Accuracy of:  0.96\n",
      "CPU times: user 45.2 ms, sys: 3.1 ms, total: 48.3 ms\n",
      "Wall time: 48.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(eta=1,\n",
    "                                  iterations=15,\n",
    "                                  C=0.001,\n",
    "                                  solver=BFGSBinaryLogisticRegression\n",
    "                                 )\n",
    "lr.fit(X,y_not_binary)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y_not_binary,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[ -7.80514785  -4.11318354   3.04014669  -3.03897197  -6.01259962]\n",
      " [ -1.00038089  -0.19967378  -1.21575964   2.30087824  -2.10311328]\n",
      " [-17.41731293  -1.86843785  -2.52460195  14.42142098  12.13502409]]\n",
      "Accuracy of:  0.98\n",
      "CPU times: user 28 ms, sys: 3.1 ms, total: 31.1 ms\n",
      "Wall time: 8.83 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(eta=1,\n",
    "                                  iterations=10,\n",
    "                                  C=0.001,\n",
    "                                  solver=HessianBinaryLogisticRegression\n",
    "                                 )\n",
    "lr.fit(X,y_not_binary)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y_not_binary,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.60162292  2.75010261 -5.75936458 -5.3884641 ]\n",
      " [-0.20102819 -1.21528251  2.30594637 -2.10682399]\n",
      " [-1.94093305 -2.68189698 15.31269536 12.86026915]]\n",
      "Accuracy of:  0.98\n",
      "CPU times: user 18 ms, sys: 3.29 ms, total: 21.3 ms\n",
      "Wall time: 18.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# how do we compare now to sklearn?\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_sk = LogisticRegression(solver='lbfgs',n_jobs=1,\n",
    "                           multi_class='ovr', C = 1/0.001, penalty='l2',\n",
    "                          max_iter=50) # all params default\n",
    "# note that sklearn is optimized for using the liblinear library with logistic regression\n",
    "# ...and its faster than our implementation here\n",
    "\n",
    "lr_sk.fit(X, y_not_binary) # no need to add bias term, sklearn does it internally!!\n",
    "print(lr_sk.coef_)\n",
    "yhat = lr_sk.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y_not_binary,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.61323935  3.3108667  -5.58974866 -5.27581511]\n",
      " [-0.20106689 -1.21528795  2.30597991 -2.10684401]\n",
      " [-1.91417603 -2.51517266 14.57415466 12.13670105]]\n",
      "Accuracy of:  0.98\n",
      "CPU times: user 3.83 ms, sys: 1.83 ms, total: 5.66 ms\n",
      "Wall time: 3.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# actually, we aren't quite as good as the lib linear implementation\n",
    "# how do we compare now to sklearn?\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_sk = LogisticRegression(solver='liblinear',n_jobs=1, \n",
    "                           multi_class='ovr', C = 1/0.001, \n",
    "                           penalty='l2',max_iter=100) \n",
    "\n",
    "lr_sk.fit(X,y_not_binary) # no need to add bias term, sklearn does it internally!!\n",
    "print(lr_sk.coef_)\n",
    "yhat = lr_sk.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y_not_binary,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liblinear is a great toolkit for linear modeling (from national Taiwan University) and the paper can be found here:\n",
    "- https://www.csie.ntu.edu.tw/~cjlin/papers/liblinear.pdf\n",
    "\n",
    "Actually, this solves a slightly different problem (known as the 'dual' formulation because its a linear SVM) to make it extremely fast. So this is actually not a fair comparison to ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# its still faster! Can we fix it with parallelization?\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class ParallelMultiClassLogisticRegression(MultiClassLogisticRegression):\n",
    "    @staticmethod\n",
    "    def par_logistic(yval,eta,iters,C,X,y,solver):\n",
    "        y_binary = y==yval # create a binary problem\n",
    "        # train the binary classifier for this class\n",
    "        hblr = solver(eta=eta,iterations=iters,C=C)\n",
    "        hblr.fit(X,y_binary)\n",
    "        return hblr\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        backend = 'threading' # can also try 'multiprocessing'\n",
    "        \n",
    "        self.classifiers_ = Parallel(n_jobs=-1,backend=backend)(\n",
    "            delayed(self.par_logistic)(yval,self.eta,self.iters,self.C,X,y,self.solver) for yval in self.unique_)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "plr = ParallelMultiClassLogisticRegression(eta=1,iterations=10,C=0.001,solver=HessianBinaryLogisticRegression)\n",
    "plr.fit(X,y_not_binary)\n",
    "print(plr)\n",
    "\n",
    "yhat = plr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y_not_binary,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Please note that the overhead of parallelization is not worth it for this problem!!\n",
    "\n",
    "**When would it make sense???**\n",
    "___\n",
    "\n",
    "\n",
    "\n",
    "# Extended Logistic Regression Example\n",
    "\n",
    "In this example we will explore methods of using logistic regression in scikit-learn. A basic understanding of scikit-learn is required to complete this notebook, but we start very basic. Note also that there are more efficient methods of separating testing and training data, but we will leave that for a later lecture.\n",
    "\n",
    "First let's load a dataset and prepare it for analysis. We will use pandas to load in data, and then prepare it for classification. We will be using the titanic dataset (a very modest sized data set of about 1000 instances). The imputation methods used here are discussed in a previous notebook.\n",
    "\n",
    "Steps:\n",
    "- Load data, impute\n",
    "- One hot encode and normalize data\n",
    "- Separate into training and testing sets\n",
    "- Explore best hyper parameter, C\n",
    "\n",
    "\n",
    "## Load Titanic Data and Pre-process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 882 entries, 0 to 890\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Survived  882 non-null    int64  \n",
      " 1   Age       882 non-null    float64\n",
      " 2   Sex       882 non-null    object \n",
      " 3   Parch     882 non-null    int64  \n",
      " 4   SibSp     882 non-null    int64  \n",
      " 5   Pclass    882 non-null    int64  \n",
      " 6   Fare      882 non-null    float64\n",
      " 7   Embarked  882 non-null    object \n",
      "dtypes: float64(2), int64(4), object(2)\n",
      "memory usage: 62.0+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericlarson/opt/anaconda3/envs/mlenv2021/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1111: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/titanic.csv') # read in the csv file\n",
    "\n",
    "# 1. Remove attributes that just arent useful for us\n",
    "del df['PassengerId']\n",
    "del df['Name']\n",
    "del df['Cabin']\n",
    "del df['Ticket']\n",
    "\n",
    "# 2. Impute some missing values, grouped by their Pclass and SibSp numbers\n",
    "df_grouped = df.groupby(by=['Pclass','SibSp'])\n",
    "\n",
    "# # now use this grouping to fill the data set in each group, then transform back\n",
    "# fill in the numeric values\n",
    "df_imputed = df_grouped.transform(lambda grp: grp.fillna(grp.median()))\n",
    "# fill in the categorical values\n",
    "df_imputed[['Sex','Embarked']] = df_grouped[['Sex','Embarked']].apply(lambda grp: grp.fillna(grp.mode()))\n",
    "# fillin the grouped variables from original data frame\n",
    "df_imputed[['Pclass','SibSp']] = df[['Pclass','SibSp']]\n",
    "\n",
    "# 4. drop rows that still had missing values after grouped imputation\n",
    "df_imputed.dropna(inplace=True)\n",
    "\n",
    "# 5. Rearrange the columns\n",
    "df_imputed = df_imputed[['Survived','Age','Sex','Parch','SibSp','Pclass','Fare','Embarked']]\n",
    "\n",
    "df_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 882 entries, 0 to 890\n",
      "Data columns (total 11 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Survived    882 non-null    int64  \n",
      " 1   Age         882 non-null    float64\n",
      " 2   Parch       882 non-null    int64  \n",
      " 3   SibSp       882 non-null    int64  \n",
      " 4   Pclass      882 non-null    int64  \n",
      " 5   Fare        882 non-null    float64\n",
      " 6   Embarked_C  882 non-null    uint8  \n",
      " 7   Embarked_Q  882 non-null    uint8  \n",
      " 8   Embarked_S  882 non-null    uint8  \n",
      " 9   IsMale      882 non-null    int64  \n",
      " 10  FamilySize  882 non-null    int64  \n",
      "dtypes: float64(2), int64(6), uint8(3)\n",
      "memory usage: 64.6 KB\n"
     ]
    }
   ],
   "source": [
    "# perform one-hot encoding of the categorical data \"embarked\"\n",
    "tmp_df = pd.get_dummies(df_imputed.Embarked,prefix='Embarked')\n",
    "df_imputed = pd.concat((df_imputed,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "# replace the current Sex atribute with something slightly more intuitive and readable\n",
    "df_imputed['IsMale'] = df_imputed.Sex=='male' \n",
    "df_imputed.IsMale = df_imputed.IsMale.astype(np.int)\n",
    "\n",
    "# Now let's clean up the dataset\n",
    "if 'Sex' in df_imputed:\n",
    "    del df_imputed['Sex'] # if 'Sex' column still exists, delete it (as we created an ismale column)\n",
    "    \n",
    "if 'Embarked' in df_imputed:    \n",
    "    del df_imputed['Embarked'] # get reid of the original category as it is now one-hot encoded\n",
    "    \n",
    "# Finally, let's create a new variable based on the number of family members\n",
    "# traveling with the passenger\n",
    "\n",
    "# notice that this new column did not exist before this line of code--we use the pandas \n",
    "#    syntax to add it in \n",
    "df_imputed['FamilySize'] = df_imputed.Parch + df_imputed.SibSp\n",
    "df_imputed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Split\n",
    "For training and testing purposes, let's gather the data we have and grab 80% of the instances for training and the remaining 20% for testing. Moreover, let's repeat this process of separating the testing and training data three times. We will use the hold out cross validation method built into scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=3, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'Survived' in df_imputed:\n",
    "    y = df_imputed['Survived'].values # get the labels we want\n",
    "    del df_imputed['Survived'] # get rid of the class label\n",
    "    norm_features = ['Age','Fare' ]\n",
    "    df_imputed[norm_features] = (df_imputed[norm_features]-df_imputed[norm_features].mean()) / df_imputed[norm_features].std()\n",
    "    X = df_imputed.to_numpy() # use everything else to predict!\n",
    "\n",
    "    ## X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    #    have converted them into simple matrices to use with scikit learn\n",
    "    \n",
    "    \n",
    "# to use the cross validation object in scikit learn, we need to grab an instance\n",
    "#    of the object and set it up. This object will be able to split our data into \n",
    "#    training and testing splits\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(\n",
    "                         n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.7909604519774012\n",
      "confusion matrix\n",
      " [[93 17]\n",
      " [20 47]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.768361581920904\n",
      "confusion matrix\n",
      " [[87 20]\n",
      " [21 49]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.8022598870056498\n",
      "confusion matrix\n",
      " [[88 17]\n",
      " [18 54]]\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "lr_clf = HessianBinaryLogisticRegression(eta=0.1,iterations=10) # get object\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "    \n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.8305084745762712\n",
      "confusion matrix\n",
      " [[99 17]\n",
      " [13 48]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.768361581920904\n",
      "confusion matrix\n",
      " [[81 12]\n",
      " [29 55]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.8305084745762712\n",
      "confusion matrix\n",
      " [[100  11]\n",
      " [ 19  47]]\n"
     ]
    }
   ],
   "source": [
    "# this does the exact same thing as the above block of code, but with shorter syntax\n",
    "\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "    lr_clf.fit(X[train_indices],y[train_indices])  # train object\n",
    "    y_hat = lr_clf.predict(X[test_indices]) # get test set precitions\n",
    "\n",
    "    # print the accuracy and confusion matrix \n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", mt.accuracy_score(y[test_indices],y_hat)) \n",
    "    print(\"confusion matrix\\n\",mt.confusion_matrix(y[test_indices],y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Example, adjusting C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4050c0fd22504788974892e350bcdc78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='cost', options=(0.0001, 0.00022758459260747887, 0.0005179474679231…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.lr_explor(cost)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import widgets as wd\n",
    "\n",
    "num_cv_iterations = 10\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.5)\n",
    "\n",
    "def lr_explor(cost):\n",
    "    print('Running')\n",
    "    lr_clf = HessianBinaryLogisticRegression(eta=0.1,iterations=10,\n",
    "                                            C=float(cost)) # get object\n",
    "    acc = []\n",
    "    for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "        lr_clf.fit(X[train_indices],y[train_indices])  # train object\n",
    "        y_hat = lr_clf.predict(X[test_indices]) # get test set predictions\n",
    "        acc.append(mt.accuracy_score(y[test_indices],y_hat))\n",
    "        \n",
    "    acc = np.array(acc)\n",
    "    print(acc.mean(),'+-',2.7*acc.std())\n",
    "        \n",
    "wd.interact(lr_explor,cost=list(np.logspace(-4,1,15)),__manual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exhaustive Search for C, then Visualize with Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.85 s, sys: 72.1 ms, total: 3.93 s\n",
      "Wall time: 4.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# alternatively, we can also graph out the values using boxplots\n",
    "num_cv_iterations = 20\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.5)\n",
    "\n",
    "def lr_explor(cost):\n",
    "    lr_clf = BFGSBinaryLogisticRegression(eta=0.1,iterations=10,\n",
    "                                            C=float(cost)) # get object\n",
    "    acc = []\n",
    "    for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "        lr_clf.fit(X[train_indices],y[train_indices])  # train object\n",
    "        y_hat = lr_clf.predict(X[test_indices]) # get test set predictions\n",
    "        acc.append(mt.accuracy_score(y[test_indices],y_hat))\n",
    "        \n",
    "    acc = np.array(acc)\n",
    "    return acc\n",
    "\n",
    "costs = np.logspace(-5,1,20)\n",
    "accs = []\n",
    "for c in costs:\n",
    "    accs.append(lr_explor(c))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now show a boxplot of the data across c\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.boxplot(accs)\n",
    "plt.xticks(range(1,len(costs)+1),['%.4f'%(c) for c in costs],rotation='vertical')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "___\n",
    "# Next Time: Neural Networks\n",
    "___\n",
    "\n",
    "In this notebook you learned:\n",
    "- Formulation of Logistic regression with different optimization strategies\n",
    " - Line Search\n",
    " - Mini-batch\n",
    " - Stochastic Gradient\n",
    "- Newton's Approach using Hessian \n",
    "- Quasi Newton's Method\n",
    "- Use Exhaustive Searches for Finding \"C\" \n",
    "- And Training/Testing Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
